{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746578a4-88dd-4cec-b437-d6f1d0865ef6",
   "metadata": {},
   "source": [
    "# Generating data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f12ed4de-a7b6-4090-9eea-13fefdce9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from hyppo.tools import ts_sim\n",
    "\n",
    "TS_SIMS = [\n",
    "    \"indep_ar\",\n",
    "    \"cross_corr_ar\",\n",
    "    \"nonlinear_process\",\n",
    "    \"extinct_gaussian_process\",\n",
    "]\n",
    "\n",
    "p = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de8db0-7809-4dd8-89a3-d3b7c098aab6",
   "metadata": {},
   "source": [
    "## Generate Experiment 1 - Independent AR(1) with increasing sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d96d14-c068-471b-b7c1-80e17621f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"1-independent_ar_n\"\n",
    "\n",
    "n = 200\n",
    "reps = 300\n",
    "\n",
    "phi = 0.5\n",
    "sigma = 1 - (phi**2)\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "datas = [ts_sim(\"indep_ar\", n, phi=phi, sigma=sigma) for _ in range(reps)]\n",
    "\n",
    "X = np.stack([data[0] for data in datas])\n",
    "Y = np.stack([data[1] for data in datas])\n",
    "\n",
    "savedict = {\n",
    "    \"X\": X,\n",
    "    \"Y\": Y,\n",
    "}\n",
    "\n",
    "# save to disk\n",
    "sp.io.savemat(f\"{p}{fname}.mat\", savedict, do_compression=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba61b56-eecf-4ea2-9ea1-cf58b704257d",
   "metadata": {},
   "source": [
    "## Generate Experiment 2 - Independent AR(1) with increasing phi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a02659-109a-451f-9d06-773e0726e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"2-independent_ar_phi\"\n",
    "\n",
    "n = 200\n",
    "reps = 300\n",
    "phis = np.arange(0.2, 1, 0.05)\n",
    "sigmas = 1 - (phis**2)\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "Xs = []\n",
    "Ys = []\n",
    "\n",
    "for (phi, sigma) in zip(phis, sigmas):\n",
    "    datas = [\n",
    "        ts_sim(\"indep_ar\", n, phi=float(phi), sigma=float(sigma)) for _ in range(reps)\n",
    "    ]\n",
    "    Xs.append(np.stack([data[0] for data in datas]))\n",
    "    Ys.append(np.stack([data[1] for data in datas]))\n",
    "\n",
    "\n",
    "X = np.stack(Xs)\n",
    "Y = np.stack(Ys)\n",
    "\n",
    "savedict = {\"X\": X, \"Y\": Y, \"phi\": phis}\n",
    "\n",
    "# save to disk\n",
    "sp.io.savemat(f\"{p}{fname}.mat\", savedict, do_compression=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d804f8d-a21c-4c3d-a5d0-dc918ebea269",
   "metadata": {},
   "source": [
    "## Generate Experiment 3 - Linear cross correlated AR(1) with increasing sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc407eb-14a6-4441-b8da-32afca1f6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"3-linear_ar\"\n",
    "\n",
    "n = 200\n",
    "reps = 300\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "datas = [ts_sim(\"cross_corr_ar\", n) for _ in range(reps)]\n",
    "\n",
    "X = np.stack([data[0] for data in datas])\n",
    "Y = np.stack([data[1] for data in datas])\n",
    "\n",
    "savedict = {\n",
    "    \"X\": X,\n",
    "    \"Y\": Y,\n",
    "}\n",
    "\n",
    "# save to disk\n",
    "sp.io.savemat(f\"{p}{fname}.mat\", savedict, do_compression=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46bc6e9-a0dc-458a-bb24-e0453e1ab000",
   "metadata": {},
   "source": [
    "## Generate Experiment 4 - Non-linearly cross correlated AR(1) with increasing sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c232a9d-5c05-4ed9-8a51-1daef8efb0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"4-nonlinear_ar\"\n",
    "\n",
    "n = 200\n",
    "reps = 300\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "datas = [ts_sim(\"nonlinear_process\", n) for _ in range(reps)]\n",
    "\n",
    "X = np.stack([data[0] for data in datas])\n",
    "Y = np.stack([data[1] for data in datas])\n",
    "\n",
    "savedict = {\n",
    "    \"X\": X,\n",
    "    \"Y\": Y,\n",
    "}\n",
    "\n",
    "# save to disk\n",
    "sp.io.savemat(f\"{p}{fname}.mat\", savedict, do_compression=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60065b-fc9e-4dbf-958c-137558de83fb",
   "metadata": {},
   "source": [
    "## Generate Experiment 5 - Non-linearly cross correlated AR(1) with increasing sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32aaef0f-8d47-4b8c-9a29-e1a749996063",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"5-extinct_gaussian\"\n",
    "\n",
    "n = 200\n",
    "reps = 300\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "datas = [ts_sim(\"extinct_gaussian_process\", n) for _ in range(reps)]\n",
    "\n",
    "X = np.stack([data[0] for data in datas])\n",
    "Y = np.stack([data[1] for data in datas])\n",
    "\n",
    "savedict = {\n",
    "    \"X\": X,\n",
    "    \"Y\": Y,\n",
    "}\n",
    "\n",
    "# save to disk\n",
    "sp.io.savemat(f\"{p}{fname}.mat\", savedict, do_compression=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf8254-3a4e-4421-a272-5a0eb81d2c37",
   "metadata": {},
   "source": [
    "# Generate Experiment 6 - Independent Vector AR(1) with increasing sample size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2899b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indep_var(n, d, phi=0.5, seed=None):\n",
    "    \"\"\"\n",
    "    d : corresponds to dimension of the time series\n",
    "    \"\"\"\n",
    "    rng = rng = np.random.default_rng(seed)\n",
    "    coeff = np.eye(d * 2) * phi\n",
    "    covar = np.eye(d * 2) * (1 - (phi**2))\n",
    "    errors = np.random.multivariate_normal(np.zeros(d * 2), covar, n)\n",
    "\n",
    "    Y = np.zeros((n, d * 2))\n",
    "    Y[0] = 0\n",
    "\n",
    "    for t in range(1, n):\n",
    "        Y[t] = np.dot(coeff, Y[t - 1]) + errors[t]\n",
    "\n",
    "    series1 = Y[:, :d]\n",
    "    series2 = Y[:, d:]\n",
    "\n",
    "    return series1, series2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"6-independent_var_n\"\n",
    "\n",
    "n = 200\n",
    "d = 100\n",
    "reps = 300\n",
    "\n",
    "phi = 0.5\n",
    "\n",
    "datas = [indep_var(n, d, phi, seed=1) for _ in range(reps)]\n",
    "\n",
    "X = np.stack([data[0] for data in datas])\n",
    "Y = np.stack([data[1] for data in datas])\n",
    "\n",
    "savedict = {\n",
    "    \"X\": X,\n",
    "    \"Y\": Y,\n",
    "}\n",
    "\n",
    "# save to disk\n",
    "sp.io.savemat(f\"{p}{fname}.mat\", savedict, do_compression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8583276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 200, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ac59d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
